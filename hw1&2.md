# hw 1&2

### Dropout 和 Batchnorm
- Dropout是在模型过拟合的时候才能派上用场，Batchnorm和Dropout并不完全冲突，不像网上的野鸡博客讲的那样，实际上Batchnorm做的事情是重新映射了数据的分布，使得数据空间更加平滑，从而使得解空间也更加平滑，梯度下降更容易找到极小值。原论文解释是减少内部协变量偏移，但这个说法也有待考证，并且不太直观。
- 而Dropout是通过随机灭活神经元，在多轮的平均对冲下减缓过拟合，二者本质完全不同。
- 但是我们要注意Dropout实际上会改变数据分布的方差，随着模型层数增加，这种影响会逐渐累积，所以在深度神经网络中也许副作用就会很严重，但是如果只是用于模型的输出层的话，效果就会很好（注意由于反向传播的原因，其实用于最后一层效果也可以扩散到整个网络）。在机器学习比赛中也可以找到使用这种trick的例子。

### 模型参数量和数据集的数量
- 模型参数量和数据集的数量是正相关的（结合欠拟合和过拟合来理解）。随着模型参数量增大，会越发容易过拟合，所以在训练的时候如果发现模型几轮下来train loss就很低了（点名LSTM），那就需要狠狠调大Dropout值，抑制模型的过拟合。
- 训练集的数量自然是越多越好，那么当你发现怎么调都差一点时，可以尝试增大训练集的比例，例如hw2，最开始是0.75，最后设置成了0.95

### 模型的输出层
- 模型的输出层其实也是很重要的。在hw2中，biLSTM最后输出的数据维度是2*hidden_dim，但是最终的output_dim是41，相差巨大。最开始我直接采用一个linear层映射到41，效果并不好。最终是改成了两个linear层，中间加入了batchnorm，relu和dropout，使得输出层更灵活，更难遗漏掉高维数据中的信息。

### 关于RNN
- RNN最终输出的是最后n个时间步的数据，而我们实际上只需要最后一个时间步的数据就够了。对于biRNN来说，需要理解其处理数据的方式，正向的RNN最后一个时间步是在序列末端，但是反向的RNN最后一个时间步却是在序列的头部，所以需要进行一个裁剪拼接（但hw2还没有正确拼出来）。如果想要方便处理的话可以直接取中间值，这个地方刚好是正反都处理了一半，信息基本都利用到了。或者可以直接把头尾拼起来，只是其中就有一半是无用信息。